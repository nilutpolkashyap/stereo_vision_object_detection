{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "import imutils\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Function for stereo vision and depth estimation\n",
    "import triangulation as tri\n",
    "import calibration\n",
    "\n",
    "# Mediapipe for face detection\n",
    "import mediapipe as mp\n",
    "import time\n",
    "\n",
    "mp_facedetector = mp.solutions.face_detection\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "\n",
    "# Open both cameras\n",
    "# cap_right = cv2.VideoCapture(2, cv2.CAP_DSHOW)                    \n",
    "# cap_left =  cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "\n",
    "\n",
    "# Stereo vision setup parameters\n",
    "frame_rate = 120    #Camera frame rate (maximum at 120 fps)\n",
    "B = 3               #Distance between the cameras [cm]\n",
    "f = 8              #Camera lense's focal length [mm]\n",
    "alpha = 56.6        #Camera field of view in the horisontal plane [degrees]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Main program loop with face detector and depth estimation using stereo vision\n",
    "with mp_facedetector.FaceDetection(min_detection_confidence=0.7) as face_detection:\n",
    "\n",
    "    while(cap_right.isOpened() and cap_left.isOpened()):\n",
    "\n",
    "        succes_right, frame_right = cap_right.read()\n",
    "        succes_left, frame_left = cap_left.read()\n",
    "\n",
    "    ################## CALIBRATION #########################################################\n",
    "\n",
    "        frame_right, frame_left = calibration.undistortRectify(frame_right, frame_left)\n",
    "\n",
    "    ########################################################################################\n",
    "\n",
    "        # If cannot catch any frame, break\n",
    "        if not succes_right or not succes_left:                    \n",
    "            break\n",
    "\n",
    "        else:\n",
    "\n",
    "            start = time.time()\n",
    "            \n",
    "            # Convert the BGR image to RGB\n",
    "            frame_right = cv2.cvtColor(frame_right, cv2.COLOR_BGR2RGB)\n",
    "            frame_left = cv2.cvtColor(frame_left, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            # Process the image and find faces\n",
    "            results_right = face_detection.process(frame_right)\n",
    "            results_left = face_detection.process(frame_left)\n",
    "\n",
    "            # Convert the RGB image to BGR\n",
    "            frame_right = cv2.cvtColor(frame_right, cv2.COLOR_RGB2BGR)\n",
    "            frame_left = cv2.cvtColor(frame_left, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "\n",
    "            ################## CALCULATING DEPTH #########################################################\n",
    "\n",
    "            center_right = 0\n",
    "            center_left = 0\n",
    "\n",
    "            if results_right.detections:\n",
    "                for id, detection in enumerate(results_right.detections):\n",
    "                    mp_draw.draw_detection(frame_right, detection)\n",
    "\n",
    "                    bBox = detection.location_data.relative_bounding_box\n",
    "\n",
    "                    h, w, c = frame_right.shape\n",
    "\n",
    "                    boundBox = int(bBox.xmin * w), int(bBox.ymin * h), int(bBox.width * w), int(bBox.height * h)\n",
    "\n",
    "                    center_point_right = (boundBox[0] + boundBox[2] / 2, boundBox[1] + boundBox[3] / 2)\n",
    "\n",
    "                    cv2.putText(frame_right, f'{int(detection.score[0]*100)}%', (boundBox[0], boundBox[1] - 20), cv2.FONT_HERSHEY_SIMPLEX, 2, (0,255,0), 2)\n",
    "\n",
    "\n",
    "            if results_left.detections:\n",
    "                for id, detection in enumerate(results_left.detections):\n",
    "                    mp_draw.draw_detection(frame_left, detection)\n",
    "\n",
    "                    bBox = detection.location_data.relative_bounding_box\n",
    "\n",
    "                    h, w, c = frame_left.shape\n",
    "\n",
    "                    boundBox = int(bBox.xmin * w), int(bBox.ymin * h), int(bBox.width * w), int(bBox.height * h)\n",
    "\n",
    "                    center_point_left = (boundBox[0] + boundBox[2] / 2, boundBox[1] + boundBox[3] / 2)\n",
    "\n",
    "                    cv2.putText(frame_left, f'{int(detection.score[0]*100)}%', (boundBox[0], boundBox[1] - 20), cv2.FONT_HERSHEY_SIMPLEX, 2, (0,255,0), 2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # If no ball can be caught in one camera show text \"TRACKING LOST\"\n",
    "            if not results_right.detections or not results_left.detections:\n",
    "                cv2.putText(frame_right, \"TRACKING LOST\", (75,50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255),2)\n",
    "                cv2.putText(frame_left, \"TRACKING LOST\", (75,50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255),2)\n",
    "\n",
    "            else:\n",
    "                # Function to calculate depth of object. Outputs vector of all depths in case of several balls.\n",
    "                # All formulas used to find depth is in video presentaion\n",
    "                depth = tri.find_depth(center_point_right, center_point_left, frame_right, frame_left, B, f, alpha)\n",
    "\n",
    "                cv2.putText(frame_right, \"Distance: \" + str(round(depth,1)), (50,50), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0,255,0),3)\n",
    "                cv2.putText(frame_left, \"Distance: \" + str(round(depth,1)), (50,50), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0,255,0),3)\n",
    "                # Multiply computer value with 205.8 to get real-life depth in [cm]. The factor was found manually.\n",
    "                print(\"Depth: \", str(round(depth,1)))\n",
    "\n",
    "\n",
    "\n",
    "            end = time.time()\n",
    "            totalTime = end - start\n",
    "\n",
    "            fps = 1 / totalTime\n",
    "            #print(\"FPS: \", fps)\n",
    "\n",
    "            cv2.putText(frame_right, f'FPS: {int(fps)}', (20,450), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0,255,0), 2)\n",
    "            cv2.putText(frame_left, f'FPS: {int(fps)}', (20,450), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0,255,0), 2)                                   \n",
    "\n",
    "\n",
    "            # Show the frames\n",
    "            cv2.imshow(\"frame right\", frame_right) \n",
    "            cv2.imshow(\"frame left\", frame_left)\n",
    "\n",
    "\n",
    "            # Hit \"q\" to close the window\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "import imutils\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Function for stereo vision and depth estimation\n",
    "import triangulation as tri\n",
    "import calibration\n",
    "\n",
    "# Mediapipe for face detection\n",
    "import mediapipe as mp\n",
    "import time\n",
    "import depthai as dai\n",
    "\n",
    "mp_facedetector = mp.solutions.face_detection\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "\n",
    "# Open both cameras\n",
    "# cap_right = cv2.VideoCapture(2, cv2.CAP_DSHOW)                    \n",
    "# cap_left =  cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "\n",
    "\n",
    "# Stereo vision setup parameters\n",
    "frame_rate = 120    #Camera frame rate (maximum at 120 fps)\n",
    "B = 4               #Distance between the cameras [cm]\n",
    "f = 80              #Camera lense's focal length [mm]\n",
    "alpha = 200      #56.6        #Camera field of view in the horisontal plane [degrees]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFrame(queue):\n",
    "    # Get frame from queue\n",
    "    frame = queue.get()\n",
    "    # Convert frame to OpenCV format and return\n",
    "    return frame.getCvFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMonoCamera(pipeline, isLeft):\n",
    "    # Configure mono camera\n",
    "    mono = pipeline.createMonoCamera()\n",
    "\n",
    "    # Set Camera Resolution\n",
    "    mono.setResolution(dai.MonoCameraProperties.SensorResolution.THE_400_P)\n",
    "    \n",
    "    if isLeft:\n",
    "        # Get left camera\n",
    "        mono.setBoardSocket(dai.CameraBoardSocket.LEFT)\n",
    "    else :\n",
    "        # Get right camera\n",
    "        mono.setBoardSocket(dai.CameraBoardSocket.RIGHT)\n",
    "    return mono"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "No available devices",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-66-c09dc39e8b2c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;31m# Pipeline is defined, now we can connect to the device\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mdai\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;31m# Get output queues.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: No available devices"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    pipeline = dai.Pipeline()\n",
    "    \n",
    "    # Set up left and right cameras\n",
    "    monoLeft = getMonoCamera(pipeline, isLeft = True)\n",
    "    monoRight = getMonoCamera(pipeline, isLeft = False)\n",
    "    \n",
    "    # Set output Xlink for left camera\n",
    "    xoutLeft = pipeline.createXLinkOut()\n",
    "    xoutLeft.setStreamName(\"left\")\n",
    "    \n",
    "    # Set output Xlink for right camera\n",
    "    xoutRight = pipeline.createXLinkOut()\n",
    "    xoutRight.setStreamName(\"right\")\n",
    "    \n",
    "    # Attach cameras to output Xlink\n",
    "    monoLeft.out.link(xoutLeft.input)\n",
    "    monoRight.out.link(xoutRight.input)\n",
    "    \n",
    "    # Pipeline is defined, now we can connect to the device\n",
    "    with dai.Device(pipeline) as device:\n",
    "        \n",
    "        # Get output queues. \n",
    "        leftQueue = device.getOutputQueue(name=\"left\", maxSize=1)\n",
    "        rightQueue = device.getOutputQueue(name=\"right\", maxSize=1)\n",
    "        \n",
    "#         with mp_facedetector.FaceDetection(min_detection_confidence=0.7) as face_detection:\n",
    "            \n",
    "        face_detection = mp_facedetector.FaceDetection(min_detection_confidence=0.7)\n",
    "        \n",
    "        while(True):\n",
    "            # Get left frame\n",
    "            frame_left = getFrame(leftQueue)\n",
    "            # Get right frame\n",
    "            frame_right = getFrame(rightQueue)\n",
    "            \n",
    "#             cv2.imshow(\"Left Frame\", leftFrame)\n",
    "#             cv2.imshow(\"Right Frame\", rightFrame)\n",
    "\n",
    "            frame_right, frame_left = calibration.undistortRectify(frame_right, frame_left)\n",
    "    \n",
    "#             start = time.time()\n",
    "        \n",
    "            # Convert the BGR image to RGB\n",
    "            frame_right = cv2.cvtColor(frame_right, cv2.COLOR_BGR2RGB)\n",
    "            frame_left = cv2.cvtColor(frame_left, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            # Process the image and find faces\n",
    "            results_right = face_detection.process(frame_right)\n",
    "            results_left = face_detection.process(frame_left)\n",
    "\n",
    "#             # Convert the RGB image to BGR\n",
    "#             frame_right = cv2.cvtColor(frame_right, cv2.COLOR_RGB2BGR)\n",
    "#             frame_left = cv2.cvtColor(frame_left, cv2.COLOR_RGB2BGR)\n",
    "                \n",
    "#             center_right = 0\n",
    "#             center_left = 0\n",
    "\n",
    "#             if results_right.detections:\n",
    "#                 for id, detection in enumerate(results_right.detections):\n",
    "#                     mp_draw.draw_detection(frame_right, detection)\n",
    "\n",
    "#                     bBox = detection.location_data.relative_bounding_box\n",
    "\n",
    "#                     h, w, c = frame_right.shape\n",
    "\n",
    "#                     boundBox = int(bBox.xmin * w), int(bBox.ymin * h), int(bBox.width * w), int(bBox.height * h)\n",
    "\n",
    "#                     center_point_right = (boundBox[0] + boundBox[2] / 2, boundBox[1] + boundBox[3] / 2)\n",
    "\n",
    "#                     cv2.putText(frame_right, f'{int(detection.score[0]*100)}%', (boundBox[0], boundBox[1] - 20), cv2.FONT_HERSHEY_SIMPLEX, 2, (0,255,0), 2)\n",
    "            \n",
    "#             if results_left.detections:\n",
    "#                 for id, detection in enumerate(results_left.detections):\n",
    "#                     mp_draw.draw_detection(frame_left, detection)\n",
    "\n",
    "#                     bBox = detection.location_data.relative_bounding_box\n",
    "\n",
    "#                     h, w, c = frame_left.shape\n",
    "\n",
    "#                     boundBox = int(bBox.xmin * w), int(bBox.ymin * h), int(bBox.width * w), int(bBox.height * h)\n",
    "\n",
    "#                     center_point_left = (boundBox[0] + boundBox[2] / 2, boundBox[1] + boundBox[3] / 2)\n",
    "\n",
    "#                     cv2.putText(frame_left, f'{int(detection.score[0]*100)}%', (boundBox[0], boundBox[1] - 20), cv2.FONT_HERSHEY_SIMPLEX, 2, (0,255,0), 2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#             # If no ball can be caught in one camera show text \"TRACKING LOST\"\n",
    "#             if not results_right.detections or not results_left.detections:\n",
    "#                 cv2.putText(frame_right, \"TRACKING LOST\", (75,50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255),2)\n",
    "#                 cv2.putText(frame_left, \"TRACKING LOST\", (75,50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255),2)\n",
    "\n",
    "#             else:\n",
    "#                 # Function to calculate depth of object. Outputs vector of all depths in case of several balls.\n",
    "#                 # All formulas used to find depth is in video presentaion\n",
    "#                 depth = tri.find_depth(center_point_right, center_point_left, frame_right, frame_left, B, f, alpha)\n",
    "\n",
    "#                 cv2.putText(frame_right, \"Distance: \" + str(round(depth,1)), (50,50), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0,255,0),3)\n",
    "#                 cv2.putText(frame_left, \"Distance: \" + str(round(depth,1)), (50,50), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0,255,0),3)\n",
    "#                 # Multiply computer value with 205.8 to get real-life depth in [cm]. The factor was found manually.\n",
    "#                 print(\"Depth: \", str(round(depth,1)))\n",
    "\n",
    "\n",
    "\n",
    "#             end = time.time()\n",
    "#             totalTime = end - start\n",
    "\n",
    "#             fps = 1 / totalTime\n",
    "#             #print(\"FPS: \", fps)\n",
    "\n",
    "#             cv2.putText(frame_right, f'FPS: {int(fps)}', (20,450), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0,255,0), 2)\n",
    "#             cv2.putText(frame_left, f'FPS: {int(fps)}', (20,450), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0,255,0), 2)                                   \n",
    "\n",
    "\n",
    "                # Show the frames\n",
    "            cv2.imshow(\"frame right\", frame_right) \n",
    "            cv2.imshow(\"frame left\", frame_left)\n",
    "\n",
    "\n",
    "                # Hit \"q\" to close the window\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "                \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfgpu",
   "language": "python",
   "name": "tfgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
